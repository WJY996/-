@[李林健小组](这里写自定义目录标题)

# 反反爬虫课程设计

----------




## 小组成员
1. 李林健
2. xxx
3. xxx
4. 刘一达


## 课程设计思考

经过讨论，我们认为此次课设的目的为让我们更多的了解爬虫-反爬虫-反反爬虫机制，了解基础的反爬虫手段，与相应的反反爬虫模式，并根据网络资料对一些经典的反反爬虫案例进行复现


## 目录
1. 简介
2. 爬虫的原理与基本过程
3. 反爬虫经典策略
4. 反反爬虫方法
5. 经典反反爬虫案例复现

----------


## 1.简介 ##
网络爬虫是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。  
而反网络爬虫则是根据ip访问频率，浏览网页速度，账户登录，输入验证码等技术阻止爬虫程序运行的程序。  
反反爬虫则是绕开反网络爬虫机制再次抓取信息的网络爬虫程序。  
举个例子，爬虫像一个小偷，要去房子里偷数据，而反爬虫则是这间住房的防护措施，小偷第一次被铁门阻挡之后选择挖地道，就是一种反反爬虫的方式。总之防护的手段多种多样，小偷的手段也层出不穷。

## 2.爬虫的原理与基本过程 ##

- 爬虫的定义
- 爬虫的应用
- 爬虫的原理
- 爬虫的基本流程

###爬虫的定义
网络爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。   
简单来说，就是自动请求网页并获取数据的程序
###爬虫的应用
爬虫的应用多种多样，从我们本专业的学习生活来讲，可以通过学习爬虫来建立或优化独立的搜索引擎，也可以使用爬虫来获取机器学习需要的大量数据，或者去爬想看的剧喜欢明星的照片等。在信息安全角度，爬虫除了页面本身信息之外，还可以获得浏览器的传输信息，从而获取用户数据。
###爬虫的原理
首先我们需要了解网上冲浪的实际过程：  
在浏览器中输入地址 —— 经过DNS服务器找到服务器主机 —— 向服务器发送一个请求 —— 服务器经过解析后发送给用户浏览器结果  
用户实际上得到的只是html的代码，即爬虫的任务就是分析这段代码，从中提取对自己有用的信息
###爬虫的基本流程
- 发起请求
通过url向服务器发起request请求，请求可以包含额外的header信息。
- 获取响应内容
如果服务器正常响应，那我们将会收到一个response，response即为我们所请求的网页内容，或许包含HTML，Json字符串或者二进制的数据（视频、图片）等。
- 解析内容
如果是HTML代码，则可以使用网页解析器进行解析，如果是Json数据，则可以转换成Json对象进行解析，如果是二进制的数据，则可以保存到文件进行进一步处理。
- 保存数据
可以保存到本地文件，也可以保存到数据库（MySQL，Redis，Mongodb等）
##3. 反爬虫经典策略
###反爬虫定义与基本方法
使用任何技术手段，阻止别人批量获取自己网站信息的一种方式。爬虫分为发起请求，获取内容，解析内容，保存数据四个步骤，基本的反爬虫方法大多针对前两个步骤。
###反爬虫经典策略
- 通过Headers进行识别与限制  
主要在爬虫发起请求时进行识别与限制。无论是浏览器还是爬虫程序，在通过url发起网络请求的时候，一定会包含头文件header。主要作用为浏览器向服务器表明身份，当发送的header不符合规范时，服务器会拒绝访问请求。
- 通过IP地址进行限制
当使用同样的ip地址短时间持续访问同一个网站时，网站后台会对你的行为进行判断，管理员会进行封锁这个ip
- 通过判断用户行为进行限制  
正常的用户不会像爬虫程序那样短时间内对网站进行高频率访问，而当爬虫程序可以模拟用户的基本行为时，例如限制访问频率请求速度等，网站也可以通过弹出验证码滑动解锁之类的方法来判断访问方是否属于真实用户
- 通过动态页面进行限制
部分网站数据通过ajax请求得到，或者通过Java生成。还可以把ajax请求的参数进行加密，封装基本功能只调用自己的接口并且对接口参数加密
- 通过robots.txt进行限制  
 这个方法更多像一个君子协议，通常只有搜索引擎爬虫会遵守规则
> robots.txt（统一小写）是一种存放于网站根目录下的ASCII编码的文本文件，它通常告诉网络搜索引擎的漫游器（又称网络蜘蛛），此网站中的哪些内容是不应被搜索引擎的漫游器获取的，哪些是可以被漫游器获取的。因为一些系统中的URL是大小写敏感的，所以robots.txt的文件名应统一为小写。robots.txt应放置于网站的根目录下。如果想单独定义搜索引擎的漫游器访问子目录时的行为，那么可以将自定的设置合并到根目录下的robots.txt，或者使用robots元数据（Metadata，又称元数据）。
robots.txt协议并不是一个规范，而只是约定俗成的，所以并不能保证网站的隐私。注意robots.txt是用字符串比较来确定是否获取URL，所以目录末尾有与没有斜杠“/”表示的是不同的URL。robots.txt允许使用类似"Disallow: *.gif"这样的通配符
###反爬虫实测
**这里我想做俩个简单爬虫，爬一下知乎网易之类的看一下反馈，可以放一下代码和网站的回馈，我在弄着**  
**蜜罐那个发现就可以放在这里**
##反反爬虫方法
###反反爬虫核心思想
反反爬虫就是一个找到反爬虫机制漏洞的方法，主要是针对反爬虫手段进行分析与绕过
###反反爬虫基本方法
- 通过Headers进行识别与限制  
可以直接在爬虫中添加Headers，例如面对Headers的User-Agent检测可以直接将浏览器的User-Agent复制到爬虫的Headers中，面对Referer检测访问域名将Referer值修改为目标网站域名，或者改成百度说
- 通过IP地址进行限制  
比较主流的方式是使用ip代理池，使你的ip访问地址不断变化，或者使用云服务来自建代理池。
- 通过判断用户行为进行限制  
对于访问频率的限制，可以在每次发出请求后增加随机时间间隔来降低并随机化访问频率。对于一些有漏洞的需要登录的网站，可以在发出一定数量的请求后，重新登录继续请求来绕过同一账号短时间内不能多次进行相同请求的限制
- 通过动态页面进行限制  
由于加密数据的手段方法各不相同，好像只能具体情况具体分析

##反反爬虫复现
**https://cloud.tencent.com/developer/article/1346012可以根据这里面提到的几个反爬取机制简单的网站做一下翻翻爬虫，我周五试试，，，**  


**然后就是老板你之前提到的自己弄一下攻防玩玩，**
